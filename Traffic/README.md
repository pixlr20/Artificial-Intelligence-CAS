My first neural network was a mimic of the convolution neural network in the Lecture notes. To my surprise, it actually worked quite well with over ninety percent accuracy. My experimentation process was at first to start just adding and removing elements one at a time to see how it would affect the neural network. The sucess of a neural network was mainly it's accuracy on the validation set since I did not want to consider the Epochs due to a risk of overfitting. I started by adding an extra convolutional and pooling layer which did not do much. This was a common theme with a lot of additions to the network. I would add extra units to a layer or add a second layer and it would not cause much change. It seems like “brute-forcing” a neural network by just increasing layers and units is not very effective. It may make the network slightly better, but there are diminishing returns to adding extra layers and units. Eventually, the benefits of more parameters gets overshadowed with how slow the neural network becomes which is an important thing to note especially for high stakes operations like autonomous driving.

Some things cause the neural network to be highly ineffective. When I first used dropout (with rate equal to 0.5), it made my neural network highly inaccurate. I thought this was due to a lack of units. When I added more units, the dropout was better but still not that good. One thing I did note was that usually the dropout neural network would be noticeably more accurate on the validation set then the training sets. However, it still seemed like the dropout was actually making my neural network too underfit in the current circumstance. I also tried changing the activation functions to things like sigmoid which at first caused the results to be inaccurate again. It appears that different activation functions are useful for different circumstances, and it is important to understand why you are choosing the activation function you have. In my section, the TF nicely pointed out how different activation functions have noticeable downsides which was helpful (like RELU having a very steep incline and not being normalized at all).

After a while of adding layers, changing activation functions, and changing units I could not make my neural network get much better than the low ninety percent. My hypothesis was that the issue was with the input itself. The input image of 30x30 was quite small so once I used convolution and pooling it didn’t leave enough information to tell the difference between signs. This makes sense because a lot of signs have generally similar shapes, but it is the details of what’s on the sign that actually gives away what it is. So, I decided to increase my input image to 50x50 and increased the number of convolution filters. Immediately, the results jumped up. I began adding more convolution layers and the results noticeably improved. I added dropout to the Dense layer and it actually helped make the validation set more accurate. By giving the neural network more information, it made much better judgments and was more responsive to my changes to the structure. Neural networks are limited by the quality of their inputs; therefore, it is highly important to consider how much information a neural network needs to be accurate with consideration for the increased processing time.
